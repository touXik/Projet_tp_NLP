# -*- coding: utf-8 -*-
"""TPP_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NBk_j5tAsWNkgYG3ga6hXgcnSU0IfBMr

## TP N°03 : Techniques Avancées de Prétraitement de Textes

### Importation des Bibliothèques
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import string

"""### Télèchargment de ressource NLTK"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

"""### Chargement du corpus"""

df =pd.read_csv("/content/all-data.csv",encoding='ISO-8859-1',header=None)
noms_colonnes = ['sentiment', 'texte']
df.columns = noms_colonnes
df.head()

# afficher les dimension du dataframe
df.shape

"""### **Fonction de prétraitement**"""

def preprocess(text):
    # Tokenisation
    tokens = word_tokenize(text)

    # Suppression de la ponctuation et des chiffres
    tokens = [word.lower() for word in tokens if word.isalpha()]

    # Suppression des stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # création du lemmatizer
    lemmatizer = WordNetLemmatizer()

    # Part-of-speech tagging
    pos_tags = pos_tag(tokens)

    # Création d'une liste de dictionnaires pour chaque token avec son lemme et sa POS
    result_list = [{'token': token, 'lemme': lemmatizer.lemmatize(token), 'pos': pos} for (token, pos) in pos_tags]

    return result_list

# Appliquation la fonction de prétraitement à la colonne 'texte' et créez une nouvelle colonne 'result'
df['result'] = df['texte'].apply(preprocess)

"""### **Transformation du Résultat en DataFrame Structuré**"""

result_list_with_empty_lines = []

for result_list in df['result']:
    # Ajouter le résultat actuel à la liste
    result_list_with_empty_lines.extend(result_list)

    # Ajouter une ligne vide à la liste
    result_list_with_empty_lines.append({'token': '', 'lemme': '', 'pos': ''})

# Supprimer la dernière ligne vide ajoutée, car elle n'est pas nécessaire à la fin
result_list_with_empty_lines.pop()

# Créer le DataFrame final
result_df = pd.DataFrame(result_list_with_empty_lines)

# Affichage du DataFrame obtenu
result_df.head(20)

"""### Exportation du DataFrame vers un Fichier CSV et Rechargement"""

chemin_fichier_csv = '/content/all-data_tp3.csv'
result_df.to_csv(chemin_fichier_csv, index=False)

"""## TP N°04 : Vectorisation de Texte avec TF-IDF et Word2Vec

### Chargement du datafram df1
"""

df1 =pd.read_csv("/content/all-data_tp3.csv")
df1.shape
df1.head()

df1.shape

"""### **1) Représentation avec la méthode TF-IDF**

#### **module TfidfVectorizer**
"""

from sklearn.feature_extraction.text import TfidfVectorizer

"""#### **Création Liste de Phrases**"""

# Créer une liste pour stocker les phrases
phrases = []

# Initialiser une variable pour stocker la phrase en cours
current_phrase = []

# Parcourir les lignes du DataFrame
for index, row in df1.iterrows():
    # Vérifier si la valeur dans la colonne "lemme" est NaN
    if pd.isna(row['lemme']):
        # Si c'est le cas, ajouter la phrase en cours à la liste des phrases
        phrases.append(' '.join(current_phrase))
        # Réinitialiser la phrase en cours
        current_phrase = []
    else:
        # Sinon, ajouter le lemme à la phrase en cours
        current_phrase.append(row['lemme'])

# Vérifier s'il reste une phrase non ajoutée à la liste
if current_phrase:
    phrases.append(' '.join(current_phrase))

# Afficher la liste des phrases
print(phrases)

len(phrases)

"""####  **Vectorisation TF-IDF**"""

# Importation de la classe TfidfVectorizer depuis scikit-learn pour la vectorisation TF-IDF
vectorizer = TfidfVectorizer()

# Transformation du corpus de phrases en une matrice TF-IDF
tfidf_matrix = vectorizer.fit_transform(phrases)

# Conversion de la matrice TF-IDF en un tableau NumPy pour une manipulation aisée
tfidf_array = tfidf_matrix.toarray()

tfidf_array

tfidf_array.shape

"""#### **creation du datafram tfidf**"""

# Les caractéristiques (mots) sont stockées dans vectorizer.get_feature_names_out()
feature_names = vectorizer.get_feature_names_out()

# Créez un DataFrame pour afficher la représentation TF-IDF
df_tfidf = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)

"""#### **DataFrame TF-IDF**"""

df_tfidf

"""### **2) Représentation avec la méthode Word2Vec**

#### preparation des données
"""

# Identifications des indices des valeurs NaN dans la colonne 'lemme'
nan_indices = df1.index[df1['lemme'].isna()].tolist()

# Regroupement des tokens en phrases à partir des indices NaN
grouped_tokens = [df1['lemme'].iloc[:nan_indices[0]].tolist()]

# Utilisation de list comprehension pour regrouper les tokens entre chaque paire d'indices NaN
grouped_tokens += [df1['lemme'].iloc[nan_indices[i] + 1:nan_indices[i + 1]].tolist() for i in range(len(nan_indices) - 1)]

# Ajout du dernier groupe de tokens après le dernier indice NaN
grouped_tokens.append(df1['lemme'].iloc[nan_indices[-1] + 1:].tolist())

# Affichage des groupes de tokens regroupés en phrases
print(grouped_tokens)

# Calcul du nombre total de groupes de tokens (représentant le nombre de phrases)
len(grouped_tokens)

"""#### Téléchargement et Exploration du modèle pré-entraîné"""

# Importation des bibliothèques nécessaires
from google.colab import drive
import gdown

# Montage de Google Drive pour accéder aux fichiers
drive.mount('/content/drive')

# Définition du chemin de sortie pour le modèle Word2Vec téléchargé
output = '/content/drive/MyDrive/Nlp/google_news.model'

# URL du modèle Word2Vec à télécharger
url = "https://drive.google.com/u/0/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download&resourcekey=0-wjGZdNAUop6WykTtMip30g"

# Téléchargement du modèle Word2Vec depuis l'URL spécifiée
gdown.download(url, output, quiet=False)

# Importation des bibliothèques nécessaires
import gzip
from gensim.models import KeyedVectors

# Chemin du modèle Word2Vec compressé
gzipped_model_path = "/content/drive/MyDrive/Nlp/google_news.model"

# Décompression du modèle Word2Vec et sauvegarde du modèle non compressé
with gzip.open(gzipped_model_path, 'rb') as f_in:
    with open("/content/drive/MyDrive/Nlp/google_news_unzipped.model", 'wb') as f_out:
        f_out.write(f_in.read())

# Chargement du modèle Word2Vec
word2vec_model = KeyedVectors.load_word2vec_format("/content/drive/MyDrive/Nlp/google_news_unzipped.model", binary=True)

# Extraction du vecteur associé au mot "king" du modèle Word2Vec
vector_king = word2vec_model['king']

# Affichage du vecteur associé au mot "king"
print("Vecteur pour 'king':", vector_king)

# Récupération de la longueur du vecteur, indiquant la dimensionnalité de l'espace vectoriel
dimensionalite_vecteur_king = len(vector_king)
print("Dimensionnalité de l'espace vectoriel pour 'king':", dimensionalite_vecteur_king)

# Recherche des mots les plus similaires au mot "king" dans le modèle Word2Vec
similar_words = word2vec_model.most_similar('king', topn=5)

# Affichage des mots similaires à "king"
print("Mots similaires à 'king':", similar_words)

# Calcul de la relation 'woman' + 'king' - 'man' dans le modèle Word2Vec
relation = word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)

# Affichage de la relation calculée
print("La relation 'woman' + 'king' - 'man' est similaire à:", relation)

"""#### **Vectorisation avec le modèle pré-entraîné**"""

# Initialisation d'une liste vide pour stocker les vecteurs de chaque phrase
vectorized_data = []

# Parcours de chaque phrase dans le corpus tokenisé (grouped_tokens)
for sentence in grouped_tokens:
    # Extraction des vecteurs des mots présents dans le modèle Word2Vec (word2vec_model)
    # et ajout de ces vecteurs à la liste sentence_vectors
    sentence_vectors = [word2vec_model[word] for word in sentence if word in word2vec_model]

    # Ajout de la liste de vecteurs de la phrase à la liste principale (vectorized_data)
    vectorized_data.append(sentence_vectors)

# Parcours de chaque phrase dans la liste des vecteurs (vectorized_data)
for i, vectors in enumerate(vectorized_data):
    # Affichage du numéro de la phrase
    print(f"Vecteurs pour la phrase {i + 1}:")

    # Parcours des mots et de leurs vecteurs associés dans la phrase actuelle
    for word, vector in zip(grouped_tokens[i], vectors):
        # Affichage du mot et de son vecteur
        print(f"{word}: {vector}")

    # Affichage d'une ligne vide pour séparer les résultats des différentes phrases
    print("\n")

# Initialisation d'une liste pour stocker les vecteurs moyens de chaque phrase
averaged_data = []

# Parcours des vecteurs de chaque phrase dans la liste des vecteurs (vectorized_data)
for sentence_vectors in vectorized_data:
    # Vérification si la liste de vecteurs n'est pas vide
    if sentence_vectors:
        # Calcul de la moyenne des vecteurs avec axis=0 pour moyenniser le long de la dimension des mots
        averaged_vector = np.mean(sentence_vectors, axis=0)
        averaged_data.append(averaged_vector)
    else:
        # Si la liste de vecteurs est vide, ajouter un vecteur nul ou une autre valeur par défaut
        averaged_data.append(np.zeros_like(vectorized_data[0][0]))

# Affichage des représentations vectorielles moyennées de chaque phrase
for i, averaged_vector in enumerate(averaged_data):
    print(f"Phrase {i + 1}: {averaged_vector}")

"""#### Data frame worde2vec"""

# Création d'un DataFrame à partir des données vectorielles moyennées
columns = [f"Feature_{i+1}" for i in range(len(averaged_data[0]))]
df_word2vec = pd.DataFrame(averaged_data, columns=columns)
df_word2vec

"""### 3) **Vectorisation des POS Tags et les Catégories**

#### encoder les pos
"""

# Importation de la classe OneHotEncoder du module sklearn.preprocessing
from sklearn.preprocessing import OneHotEncoder

# Sélection des données de la colonne 'pos' du DataFrame df1
pos_data = df1[['pos']]

# Initialisation d'un encodeur one-hot avec l'option sparse=False pour obtenir une matrice dense
encoder = OneHotEncoder(sparse=False)

# Encodage one-hot des données de la colonne 'pos'
pos_encoded = encoder.fit_transform(pos_data)

# Création d'un DataFrame (df_pos_encoded) à partir de la matrice encodée, avec des noms de colonnes appropriés
df_pos_encoded = pd.DataFrame(pos_encoded, columns=encoder.get_feature_names_out(['pos']))

# Concaténation du DataFrame original (df1) avec le DataFrame encodé (df_pos_encoded) le long de l'axe des colonnes
df1_encoded = pd.concat([df1, df_pos_encoded], axis=1)

# Affichage du DataFrame résultant
df1_encoded

"""#### df_pos"""

# Groupement du DataFrame df1_encoded en fonction des phrases (groupement par 'pos_nan')
grouped_df = df1_encoded.groupby(df1_encoded['pos_nan'].eq(1).cumsum())

# Calcul de la somme des valeurs pour chaque groupe (phrase)
df_pos = grouped_df.sum()

# Réinitialisation des index pour obtenir un DataFrame avec un index continu
df_pos.reset_index(drop=True, inplace=True)

# Affichage du DataFrame résultant (df_pos)
df_pos

"""#### la colonne sentiment"""

# Création d'un DataFrame 'sentiment' à partir de la colonne 'sentiment' du DataFrame df
sentiment = pd.DataFrame(df['sentiment'])

# Comptage des occurrences de chaque valeur dans la colonne 'sentiment'
sentiment_counts = sentiment['sentiment'].value_counts()

# Affichage du résultat du comptage
sentiment_counts

# Remplacement des labels de la colonne 'sentiment' par des valeurs numériques
sentiment['sentiment'] = df['sentiment'].replace({'positive': 1, 'negative': -1, 'neutral': 0})

# Comptage des occurrences des valeurs numériques dans la colonne 'sentiment'
sentiment_counts_numeric = sentiment['sentiment'].value_counts()

# Affichage du résultat du comptage
sentiment_counts_numeric

"""#### df_sentiment"""

sentiment

"""## TP N°05 : Analyse de Sentiments

### importation des Bibliotheque
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

"""### TF IDF"""

# Fusion des vecteurs TF-IDF avec la colonne 'sentiment' dans le dataframe
df = pd.merge(df_tfidf, sentiment, left_index=True, right_index=True)

# Séparation des données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(
    df_tfidf,  # Utilisation des vecteurs TF-IDF
    sentiment['sentiment'],  # Les étiquettes de sentiment
    test_size=0.2,  # Proportion des données à utiliser pour le test
    random_state=42  # Pour assurer la reproductibilité des résultats
)

# Initialisation d'un modèle de régression logistique
model = LogisticRegression()

# Entraînement du modèle sur l'ensemble d'entraînement
model.fit(X_train, y_train)

# Prédiction des labels pour l'ensemble de test
predictions = model.predict(X_test)

# Calcul de l'accuracy du modèle
accuracy = accuracy_score(y_test, predictions)

# Obtention du rapport de classification (précision, rappel, f1-score, etc.)
report = classification_report(y_test, predictions)

# Affichage des résultats
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

"""### Word2Vec"""

# Fusion des vecteurs Word2Vec avec les POS tags et la colonne 'sentiment' dans le dataframe
df = pd.merge(df_word2vec, sentiment, left_index=True, right_index=True)

# Séparation des données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(
    df_word2vec,  # Utilisation des vecteurs Word2Vec combinés avec les POS tags
    sentiment['sentiment'],  # Les étiquettes de sentiment
    test_size=0.2,  # Proportion des données à utiliser pour le test
    random_state=42  # Pour assurer la reproductibilité des résultats
)

# Initialisation d'un modèle de régression logistique
model = LogisticRegression()

# Entraînement du modèle sur l'ensemble d'entraînement
model.fit(X_train, y_train)

# Prédiction des labels pour l'ensemble de test
predictions = model.predict(X_test)

# Calcul de l'accuracy du modèle
accuracy = accuracy_score(y_test, predictions)

# Obtention du rapport de classification (précision, rappel, f1-score, etc.)
report = classification_report(y_test, predictions)

# Affichage des résultats
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

"""### TFIDF avec POS"""

# Concaténation des vecteurs TF-IDF avec les POS tags dans le dataframe df_tfidf_pos
df_tfidf_pos = pd.concat([df_tfidf, df_pos], axis=1)
df_tfidf_pos

# Fusion du dataframe df_tfidf_pos avec la colonne 'sentiment'
df = pd.merge(df_tfidf_pos, sentiment, left_index=True, right_index=True)

# Division des données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(
    df_tfidf_pos,
    sentiment['sentiment'],
    test_size=0.2,
    random_state=42
)

# Initialisation et entraînement du modèle de régression logistique
model = LogisticRegression()
model.fit(X_train, y_train)

# Prédictions sur l'ensemble de test
predictions = model.predict(X_test)

# Calcul de l'accuracy et du rapport de classification
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)

# Affichage des résultats
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

"""### WORD2VEC avec POS"""

# Concaténation des DataFrames df_word2vec et df_pos le long de l'axe des colonnes
df_word2vec_pos = pd.concat([df_word2vec, df_pos], axis=1)

# Affichage du nouveau DataFrame résultant de la concaténation
df_word2vec_pos

# Fusion des DataFrames df_word2vec_pos et sentiment en fonction des index
df = pd.merge(df_word2vec_pos, sentiment, left_index=True, right_index=True)

# Division des données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(
    df_word2vec_pos,
    sentiment['sentiment'],
    test_size=0.2,
    random_state=42
)

# Initialisation du modèle de régression logistique
model = LogisticRegression()

# Entraînement du modèle sur l'ensemble d'entraînement
model.fit(X_train, y_train)

# Prédictions sur l'ensemble de test
predictions = model.predict(X_test)

# Calcul de la précision du modèle
accuracy = accuracy_score(y_test, predictions)

# Génération du rapport de classification
report = classification_report(y_test, predictions)

# Affichage des résultats
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")